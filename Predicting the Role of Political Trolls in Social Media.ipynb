{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "# from transformers import BertModel, BertConfig, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import gensim\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking available devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0', '/device:XLA_CPU:0', '/device:XLA_GPU:0']\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_devices():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "\n",
    "print(get_available_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (10,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (0,15,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "filenames = glob.glob(\"russian-troll-tweets/*.csv\")\n",
    "\n",
    "li = []\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "    \n",
    "dfs = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>region</th>\n",
       "      <th>language</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>harvested_date</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>updates</th>\n",
       "      <th>...</th>\n",
       "      <th>account_type</th>\n",
       "      <th>retweet</th>\n",
       "      <th>account_category</th>\n",
       "      <th>new_june_2018</th>\n",
       "      <th>alt_external_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>article_url</th>\n",
       "      <th>tco1_step1</th>\n",
       "      <th>tco2_step1</th>\n",
       "      <th>tco3_step1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 19:58</td>\n",
       "      <td>10/1/2017 19:59</td>\n",
       "      <td>1052</td>\n",
       "      <td>9636</td>\n",
       "      <td>253</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914580356430536707</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/10_gop/status/914580356430...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 22:43</td>\n",
       "      <td>10/1/2017 22:43</td>\n",
       "      <td>1054</td>\n",
       "      <td>9637</td>\n",
       "      <td>254</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914621840496189440</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/damienwoody/status/9145685...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 22:50</td>\n",
       "      <td>10/1/2017 22:51</td>\n",
       "      <td>1054</td>\n",
       "      <td>9637</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914623490375979008</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/10_gop/status/913231923715...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 23:52</td>\n",
       "      <td>10/1/2017 23:52</td>\n",
       "      <td>1062</td>\n",
       "      <td>9642</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914639143690555392</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/10_gop/status/914639143690...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 2:13</td>\n",
       "      <td>10/1/2017 2:13</td>\n",
       "      <td>1050</td>\n",
       "      <td>9645</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914312219952861184</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/914...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   external_author_id  author  \\\n",
       "0  906000000000000000  10_GOP   \n",
       "1  906000000000000000  10_GOP   \n",
       "2  906000000000000000  10_GOP   \n",
       "3  906000000000000000  10_GOP   \n",
       "4  906000000000000000  10_GOP   \n",
       "\n",
       "                                             content   region language  \\\n",
       "0  \"We have a sitting Democrat US Senator on tria...  Unknown  English   \n",
       "1  Marshawn Lynch arrives to game in anti-Trump s...  Unknown  English   \n",
       "2  Daughter of fallen Navy Sailor delivers powerf...  Unknown  English   \n",
       "3  JUST IN: President Trump dedicates Presidents ...  Unknown  English   \n",
       "4  19,000 RESPECTING our National Anthem! #StandF...  Unknown  English   \n",
       "\n",
       "      publish_date   harvested_date  following  followers  updates  ...  \\\n",
       "0  10/1/2017 19:58  10/1/2017 19:59       1052       9636      253  ...   \n",
       "1  10/1/2017 22:43  10/1/2017 22:43       1054       9637      254  ...   \n",
       "2  10/1/2017 22:50  10/1/2017 22:51       1054       9637      255  ...   \n",
       "3  10/1/2017 23:52  10/1/2017 23:52       1062       9642      256  ...   \n",
       "4   10/1/2017 2:13   10/1/2017 2:13       1050       9645      246  ...   \n",
       "\n",
       "  account_type retweet  account_category new_june_2018     alt_external_id  \\\n",
       "0        Right       0        RightTroll             0  905874659358453760   \n",
       "1        Right       0        RightTroll             0  905874659358453760   \n",
       "2        Right       1        RightTroll             0  905874659358453760   \n",
       "3        Right       0        RightTroll             0  905874659358453760   \n",
       "4        Right       1        RightTroll             0  905874659358453760   \n",
       "\n",
       "             tweet_id                                        article_url  \\\n",
       "0  914580356430536707  http://twitter.com/905874659358453760/statuses...   \n",
       "1  914621840496189440  http://twitter.com/905874659358453760/statuses...   \n",
       "2  914623490375979008  http://twitter.com/905874659358453760/statuses...   \n",
       "3  914639143690555392  http://twitter.com/905874659358453760/statuses...   \n",
       "4  914312219952861184  http://twitter.com/905874659358453760/statuses...   \n",
       "\n",
       "                                          tco1_step1 tco2_step1 tco3_step1  \n",
       "0  https://twitter.com/10_gop/status/914580356430...        NaN        NaN  \n",
       "1  https://twitter.com/damienwoody/status/9145685...        NaN        NaN  \n",
       "2  https://twitter.com/10_gop/status/913231923715...        NaN        NaN  \n",
       "3  https://twitter.com/10_gop/status/914639143690...        NaN        NaN  \n",
       "4  https://twitter.com/realDonaldTrump/status/914...        NaN        NaN  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2946207 entries, 0 to 2946206\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   external_author_id  object\n",
      " 1   author              object\n",
      " 2   content             object\n",
      " 3   region              object\n",
      " 4   language            object\n",
      " 5   publish_date        object\n",
      " 6   harvested_date      object\n",
      " 7   following           int64 \n",
      " 8   followers           int64 \n",
      " 9   updates             int64 \n",
      " 10  post_type           object\n",
      " 11  account_type        object\n",
      " 12  retweet             int64 \n",
      " 13  account_category    object\n",
      " 14  new_june_2018       int64 \n",
      " 15  alt_external_id     object\n",
      " 16  tweet_id            int64 \n",
      " 17  article_url         object\n",
      " 18  tco1_step1          object\n",
      " 19  tco2_step1          object\n",
      " 20  tco3_step1          object\n",
      "dtypes: int64(6), object(15)\n",
      "memory usage: 472.0+ MB\n"
     ]
    }
   ],
   "source": [
    "dfs.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keeping English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2116867 entries, 0 to 2946181\n",
      "Data columns (total 21 columns):\n",
      " #   Column              Dtype \n",
      "---  ------              ----- \n",
      " 0   external_author_id  object\n",
      " 1   author              object\n",
      " 2   content             object\n",
      " 3   region              object\n",
      " 4   language            object\n",
      " 5   publish_date        object\n",
      " 6   harvested_date      object\n",
      " 7   following           int64 \n",
      " 8   followers           int64 \n",
      " 9   updates             int64 \n",
      " 10  post_type           object\n",
      " 11  account_type        object\n",
      " 12  retweet             int64 \n",
      " 13  account_category    object\n",
      " 14  new_june_2018       int64 \n",
      " 15  alt_external_id     object\n",
      " 16  tweet_id            int64 \n",
      " 17  article_url         object\n",
      " 18  tco1_step1          object\n",
      " 19  tco2_step1          object\n",
      " 20  tco3_step1          object\n",
      "dtypes: int64(6), object(15)\n",
      "memory usage: 355.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dfs = dfs[dfs['language'] == 'English']\n",
    "dfs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Right         704953\n",
       "local         458012\n",
       "Left          422141\n",
       "Hashtager     236092\n",
       "news          138581\n",
       "Commercial    112580\n",
       "Russian        20015\n",
       "Koch           10759\n",
       "?               6945\n",
       "German          3567\n",
       "Italian         1799\n",
       "Arabic          1030\n",
       "ZAPOROSHIA       172\n",
       "Spanish           79\n",
       "French            70\n",
       "Ebola             70\n",
       "Portuguese         2\n",
       "Name: account_type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['account_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RightTroll      704953\n",
       "NewsFeed        596593\n",
       "LeftTroll       422141\n",
       "HashtagGamer    236092\n",
       "Commercial      112580\n",
       "NonEnglish       26562\n",
       "Fearmonger       11001\n",
       "Unknown           6945\n",
       "Name: account_category, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['account_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2116866, 21)\n"
     ]
    }
   ],
   "source": [
    "dfs.dropna(how='any', subset=['author', 'content'], inplace=True)\n",
    "print(dfs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating 'hashtags' and 'mentions'  columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### temp_df with 'author' and 'content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author                                            content\n",
       "0  10_GOP  \"We have a sitting Democrat US Senator on tria...\n",
       "1  10_GOP  Marshawn Lynch arrives to game in anti-Trump s...\n",
       "2  10_GOP  Daughter of fallen Navy Sailor delivers powerf...\n",
       "3  10_GOP  JUST IN: President Trump dedicates Presidents ...\n",
       "4  10_GOP  19,000 RESPECTING our National Anthem! #StandF..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = dfs[['author', 'content']]\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'hashtags' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Collecting hashtags from content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>[#BoycottNFL]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>[#StandForOurAnthemðŸ‡ºðŸ‡¸]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author                                            content  \\\n",
       "0  10_GOP  \"We have a sitting Democrat US Senator on tria...   \n",
       "1  10_GOP  Marshawn Lynch arrives to game in anti-Trump s...   \n",
       "2  10_GOP  Daughter of fallen Navy Sailor delivers powerf...   \n",
       "3  10_GOP  JUST IN: President Trump dedicates Presidents ...   \n",
       "4  10_GOP  19,000 RESPECTING our National Anthem! #StandF...   \n",
       "\n",
       "                 hashtags  \n",
       "0                      []  \n",
       "1                      []  \n",
       "2           [#BoycottNFL]  \n",
       "3                      []  \n",
       "4  [#StandForOurAnthemðŸ‡ºðŸ‡¸]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df['hashtags'] = dfs['content'].str.findall(r'#.*?(?=\\s|$)')\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 'mentions' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Collecting mentions from content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[@nedryun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>[#BoycottNFL]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>[#StandForOurAnthemðŸ‡ºðŸ‡¸]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   author                                            content  \\\n",
       "0  10_GOP  \"We have a sitting Democrat US Senator on tria...   \n",
       "1  10_GOP  Marshawn Lynch arrives to game in anti-Trump s...   \n",
       "2  10_GOP  Daughter of fallen Navy Sailor delivers powerf...   \n",
       "3  10_GOP  JUST IN: President Trump dedicates Presidents ...   \n",
       "4  10_GOP  19,000 RESPECTING our National Anthem! #StandF...   \n",
       "\n",
       "                 hashtags    mentions  \n",
       "0                      []  [@nedryun]  \n",
       "1                      []          []  \n",
       "2           [#BoycottNFL]          []  \n",
       "3                      []          []  \n",
       "4  [#StandForOurAnthemðŸ‡ºðŸ‡¸]          []  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df['mentions'] = dfs['content'].str.findall(r'@.*?(?=\\s|$)')\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph requirements\n",
    "\n",
    "- U2H\n",
    " + dictionary contains {'author' : list of all his unique hashtags}\n",
    " + List of unique authors\n",
    " + List of unique hashtags\n",
    " \n",
    "- U2M\n",
    " + dictionary contains {'author' : list of all his unique mentions}\n",
    " + List of unique authors\n",
    " + List of unique mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U2H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating dictionary contains {'author' : list of all his unique hashtags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2115095/2116866 [00:26<00:00, 81764.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2116866/2116866 [00:26<00:00, 79616.15it/s]\n"
     ]
    }
   ],
   "source": [
    "hashtags_dic = dict()\n",
    "for i in tqdm(range(temp_df.shape[0]), position=0):\n",
    "    hashtags_dic.setdefault(temp_df.iloc[i, 0], []).append(temp_df.iloc[i, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author:  10_GOP\n",
      "\n",
      "hashtags:  [[], [], ['#BoycottNFL'], [], ['#StandForOurAnthemðŸ‡ºðŸ‡¸'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['#HipHopAwards'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['#Iran'], [], [], ['#MAGA'], [], [], [], ['#WomenBoycottTwitter'], [], [], [], [], [], [], [], [], [], [], ['#FakeNews'], [], [], [], [], ['#LasVegas'], ['#LasVegas.'], [], ['#LasVegasShooting'], [], ['#LasVegasStrongðŸ‡ºðŸ‡¸'], ['#LasVegasShooting'], [], [], ['#VegasStrong'], [], ['#VegasStrong'], ['#BoycottNFL'], ['#SanJuan', '#TakeAKnee'], [], ['#FakeNews'], [], [], [], [], ['#PRStrong'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['#NFLBoycott', '#MAGA'], [], [], [], [], [], [], [], [], ['#IrmaHurricane2017'], ['#HurrcaneIrma'], [], ['#FightCensorship', '#FreeSpeech', '#LiberalAttack'], ['#HurricaneIrma', '#Irma', '#TampaReady', '#FLORIDAHURRICANE'], ['#Irma', '#IrmaHurricane2017'], [], ['#HurricaneIrma', '#HurrcaneIrma'], ['#FreeTennessee'], ['#FreeTennessee'], ['#60Minutes'], [], [], [], ['#IRMA'], ['#Irma', '#AfterIrma', '#TropicalStormIrma'], ['#AfterIrma', '#TropicalStormIrma'], [], [], [], ['#60Minutes'], ['#NeverForget', '#NeverForget911', '#September11th'], [], [], [], [], [], ['#FreeTennessee'], [], ['#WhatHappened'], [], [], [], [], [], [], [], [], [], [], [], ['#DACADeal'], [], [], [], ['#Berkeley'], ['#BenAtBerkeley'], ['#BenAtBerkeley'], ['#BenAtBerkeley'], [], [], [], [], [], [], [], [], [], [], [], [], ['#Maddow'], [], [], [], [], ['#MOAR.'], ['#STLVerdict'], ['#STLVerdict'], [], ['#MOAR'], ['#MOAR.'], [], ['#STLVerdict'], ['#STLVerdict'], [], ['#STLVerdict'], ['#STLVerdict'], ['#STLVerdict'], [], [], [], ['#ConstitutionDay'], ['#ConstitutionDay'], [], [], [], ['#CrookedHillary'], [], ['#EMMY2017'], [], [], [], [], [], [], [], [], ['#Manafort'], [], [], [], [], ['#Manafort'], [], [], [], [], ['#Manafort'], [], [], ['#PrayForMexico'], ['#PrayForMexico'], [], ['#PrayForMexico'], [], [], [], [], [], [], [], [], [], [], ['#Manafort'], [], [], [], [], [], [], [], [], [], [], [], [\"#AmericaFirst'\"], [], [], [], [], [], [], [\"#MAGA'\"], [], [], [], [], [], [], [], [], [], [], [], [], [], ['#Manafort'], ['#TuesdayThoughts', '#dogs', '#JJNonPoliticalTweet'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['#dotard'], [], [], [], [], [], [], [], [], ['#HerpesHillary'], [], [], [], ['#ColinKaepernick:'], [], [], [], [], [], ['#WeStand', '#NRA'], ['#FakeNews'], [], [], [], ['#CNNisFakeNews'], [], [\"#MAGA'\"], ['#TaxReform.'], [], [], ['#BoycottNFL'], [], [], [], [], [], [], [], [], [], ['#TeamScalise'], [], [], [], [], [], [], [], [], [], [], [], [], ['#FakeNews'], ['#FakeNews'], [], [], ['#FreeTennessee'], [], [], ['#FreeSpeech'], [], ['#FEMA'], ['#FreeSpeech', '#FreeTennessee'], [], [], [], [], [], [], ['#1A'], ['#1A'], [], [], ['#FreeTennessee'], [], [], ['#TrumpTrain'], [], [], ['#FreeSpeech'], ['#1A'], ['#IfIWerePresidentForOneDay', '#FreeTennessee'], ['#IfIWerePresidentForOneDay'], ['#IfIWerePresidentForOneDay'], [], ['#FreeTennessee']]\n",
      "\n",
      "# hashtags: 372\n"
     ]
    }
   ],
   "source": [
    "print(\"author: \", list(hashtags_dic.keys())[0])\n",
    "print(\"\\nhashtags: \", list(hashtags_dic.values())[0])\n",
    "print(\"\\n# hashtags:\", len(list(hashtags_dic.values())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Flatting the hashtags lists for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(hashtags_dic.items()):\n",
    "    hashtags_dic[k] = set([item for sublist in list(hashtags_dic.values())[i] for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author:  10_GOP\n",
      "\n",
      "hashtags:  {'#IRMA', '#LasVegas', '#LasVegasStrongðŸ‡ºðŸ‡¸', '#Manafort', '#FEMA', '#EMMY2017', '#TropicalStormIrma', '#dotard', '#TrumpTrain', '#TakeAKnee', '#WeStand', '#PrayForMexico', '#MOAR.', '#MOAR', '#dogs', '#NRA', '#NeverForget', '#FightCensorship', '#HipHopAwards', '#HurrcaneIrma', '#Irma', '#FreeSpeech', '#LasVegasShooting', '#WhatHappened', '#DACADeal', '#Iran', '#NFLBoycott', '#60Minutes', '#NeverForget911', '#TuesdayThoughts', '#ColinKaepernick:', '#HurricaneIrma', '#CNNisFakeNews', '#HerpesHillary', '#BoycottNFL', '#LiberalAttack', '#TaxReform.', '#IrmaHurricane2017', '#BenAtBerkeley', \"#MAGA'\", '#Maddow', '#CrookedHillary', '#1A', '#AfterIrma', '#IfIWerePresidentForOneDay', '#MAGA', '#FLORIDAHURRICANE', '#TampaReady', '#TeamScalise', '#StandForOurAnthemðŸ‡ºðŸ‡¸', '#FreeTennessee', '#JJNonPoliticalTweet', '#WomenBoycottTwitter', '#LasVegas.', '#PRStrong', '#STLVerdict', '#SanJuan', \"#AmericaFirst'\", '#Berkeley', '#September11th', '#FakeNews', '#ConstitutionDay', '#VegasStrong'}\n",
      "\n",
      "# hashtags: 63\n"
     ]
    }
   ],
   "source": [
    "print(\"author: \", list(hashtags_dic.keys())[0])\n",
    "print(\"\\nhashtags: \", list(hashtags_dic.values())[0])\n",
    "print(\"\\n# hashtags:\", len(list(hashtags_dic.values())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a list with all unique hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_list = [item for sublist in list(temp_df['hashtags']) for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457604\n",
      "110097\n"
     ]
    }
   ],
   "source": [
    "print(len(hashtags_list))\n",
    "unique_hashtags_list = set(hashtags_list)\n",
    "print(len(unique_hashtags_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U2M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating dictionary contains {'author' : list of all his unique mentions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2116866/2116866 [00:26<00:00, 80614.01it/s]\n"
     ]
    }
   ],
   "source": [
    "mentions_dic = dict()\n",
    "for i in tqdm(range(temp_df.shape[0]), position=0):\n",
    "    mentions_dic.setdefault(temp_df.iloc[i, 0], []).append(temp_df.iloc[i, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author:  10_GOP\n",
      "\n",
      "hashtags:  [['@nedryun'], [], [], [], [], [], [], ['@SenatorMenendez', '@CarmenYulinCruz'], [], ['@CNN'], ['@CNN'], ['@thehill'], [], ['@MichelleObama,'], ['@MichelleObama,'], [], ['@FLOTUS!'], [], ['@Breaking911'], [], [], [], [], [], [], [], [], [], [], [], [], ['@realDonaldTrump'], ['@realDonaldTrump', '@POTUS'], [], [], [], [], [], [], [], [], [], [], [], ['@Joy_Villa'], [], [], [], [], ['@Joy_Villa:'], [], [], [], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['@Cernovich', '@KFILE'], [], [], [], [], [], [], [], [], ['@CNN'], [], ['@AP_Politics'], ['@POTUS', '@FLOTUS'], [], [], [], [], [], ['@PrisonPlanet'], ['@PrisonPlanet', '@GrrrGraphics,one'], [], [], [], [], [], [], [], [], [], [], ['@YouTube', '@TeamYouTube,'], ['@SteveScalise'], [], [], [], [], [], [], [], [], ['@10_gop'], [], [], ['@10_gop'], [], [], [], [], [], [], ['@10_gop'], [], [], [], [], ['@10_gop'], [], [], [], [], [], [], ['@SebGorka'], [], [], ['@LauraLoomer', '@HillaryClinton'], [], [], [], [], ['@SebGorka'], [], [], [], [], ['@JackPosobiec'], [], ['@realDonaldTrump'], [], [], [], [], ['@benshapiro'], [], [], [], [], [], ['@FoxNews', '@FLOTUS'], ['@NaeguNugu', '@TEN_GOP'], ['@jojoh888'], [], [], [], ['@5sahandful', '@TEN_GOP'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], [], [], [], [], [], [], [], [], ['@realDonaldTrump'], [], [], [], [], [], ['@SLMPD'], [], [], [], ['@Breaking911'], [], ['@exposes_racism'], [], [], [], ['@thehill'], [], ['@realDonaldTrump'], [], [], [], ['@JackPosobiec'], [], [], [], [], [], [], [], ['@ChaVonZee16'], ['@ChaVonZee16'], [], [], ['@brianstelter,'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['@netanyahu'], [], ['@realDonaldTrump', '@UN'], ['@realDonaldTrump', '@UN'], ['@realDonaldTrump', '@UN'], [], ['@realDonaldTrump'], [], [], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], [], [], [], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], [], [], [], ['@10_gop', '@realDonaldTrump'], ['@10_gop', '@realDonaldTrump'], ['@realDonaldTrump'], [], [], [], ['@brianstelter,'], [], ['@larryelder'], [], ['@Breaking911'], ['@realDonaldTrump'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['@wahrbear'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], ['@realDonaldTrump'], [], [], [], [], [], ['@realDonaldTrump'], ['@realDonaldTrump'], ['@realDonaldTrump'], [], ['@realDonaldTrump'], [], [], [], [], [], [], [], [], [], [], ['@SteveScalise!'], [], ['@realDonaldTrump', '@ShopFloorNAM'], [], [], [], [], ['@realDonaldTrump'], [], [], [], ['@RedPillBlack:'], ['@realDonaldTrump'], [], [], [], [], [], ['@10_gop.'], [], [], ['@10_gop'], [], [], [], ['@Marines4MAGA'], ['@Thewall77515902', '@TEN_GOP', '@ELEVEN_GOP', '@realTEN_GOP'], ['@umknicken'], [], [], [], [], [], ['@Roni_K_Patriot'], [], [], [], ['@10_gop', '@GIT_ER_DONE_USA', '@DonnaShelley2'], ['@tamaraleighllc', '@TEN_GOP', '@ELEVEN_GOP', '@realTEN_GOP'], [], [], [], [], [], [], [], []]\n",
      "\n",
      "# hashtags: 372\n"
     ]
    }
   ],
   "source": [
    "print(\"author: \", list(mentions_dic.keys())[0])\n",
    "print(\"\\nhashtags: \", list(mentions_dic.values())[0])\n",
    "print(\"\\n# hashtags:\", len(list(mentions_dic.values())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Flatting the mentions lists for each author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(mentions_dic.items()):\n",
    "    mentions_dic[k] = set([item for sublist in list(mentions_dic.values())[i] for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author:  10_GOP\n",
      "\n",
      "hashtags:  {'@HillaryClinton', '@NaeguNugu', '@netanyahu', '@5sahandful', '@AP_Politics', '@Joy_Villa', '@Cernovich', '@Thewall77515902', '@Joy_Villa:', '@PrisonPlanet', '@LauraLoomer', '@tamaraleighllc', '@RedPillBlack:', '@GIT_ER_DONE_USA', '@realDonaldTrump', '@POTUS', '@SteveScalise', '@SLMPD', '@DonnaShelley2', '@wahrbear', '@UN', '@ELEVEN_GOP', '@realTEN_GOP', '@FoxNews', '@nedryun', '@Roni_K_Patriot', '@jojoh888', '@Marines4MAGA', '@umknicken', '@10_gop.', '@benshapiro', '@SenatorMenendez', '@SteveScalise!', '@exposes_racism', '@CNN', '@ShopFloorNAM', '@ChaVonZee16', '@Breaking911', '@GrrrGraphics,one', '@brianstelter,', '@10_gop', '@YouTube', '@FLOTUS', '@CarmenYulinCruz', '@thehill', '@MichelleObama,', '@KFILE', '@JackPosobiec', '@larryelder', '@TEN_GOP', '@FLOTUS!', '@SebGorka', '@TeamYouTube,'}\n",
      "\n",
      "# hashtags: 53\n"
     ]
    }
   ],
   "source": [
    "print(\"author: \", list(mentions_dic.keys())[0])\n",
    "print(\"\\nhashtags: \", list(mentions_dic.values())[0])\n",
    "print(\"\\n# hashtags:\", len(list(mentions_dic.values())[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Creating a list with all unique mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions_list = [item for sublist in list(temp_df['mentions']) for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043278\n",
      "566628\n"
     ]
    }
   ],
   "source": [
    "print(len(mentions_list))\n",
    "unique_mentions_list = set(mentions_list)\n",
    "print(len(unique_mentions_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contructing U2H and U2M graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# U2H = nx.Graph()\n",
    "U2M = nx.Graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -  'author' nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U2H.add_nodes_from(hashtags_dic.keys())\n",
    "U2M.add_nodes_from(mentions_dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - 'hashtags' and 'mentions' nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U2H.add_nodes_from(unique_hashtags_list)\n",
    "U2M.add_nodes_from(unique_mentions_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - U2H edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(hashtags_dic.items()):\n",
    "    for h in v:\n",
    "        U2H.add_edge(k, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - U2M edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(mentions_dic.items()):\n",
    "    for h in v:\n",
    "        U2M.add_edge(k, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting U2H and U2M graphs\n",
    "We give node2vec.Node2Vec a networkx.Graph instance, and after using .fit() (which accepts any parameter accepted by we get a gensim.models.Word2Vec) we get in return a gensim.models.Word2Vec instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U2H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Creating node2vec instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate walks\n",
    "node2vec = Node2Vec(U2H, dimensions=128, walk_length=10, num_walks=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn embeddings \n",
    "model = node2vec.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"U2H_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"U2H_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Inspect the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Yemen\n",
      "#US\n",
      "#Libya\n",
      "#Aleppo\n",
      "#SaudiArabia\n",
      "#UN\n",
      "#Putin\n",
      "#NASA\n",
      "#Palestine\n",
      "#UAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "for node, _ in model.most_similar('#Saudi'):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dic = dict()\n",
    "for i, (k, v) in enumerate(hashtags_dic.items()):\n",
    "    vec_dic[k] = model.wv.get_vector(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.5027815e-01,  1.7860985e+00, -9.9835724e-02, -1.4523559e+00,\n",
       "       -8.4916103e-01, -2.6196465e+00, -1.2881172e+00, -1.0082791e+00,\n",
       "        1.4479263e+00,  2.5722644e+00, -2.6185408e+00, -1.1167067e+00,\n",
       "       -8.0396116e-01, -1.9550753e-01, -9.2765123e-01, -1.4171256e+00,\n",
       "       -2.4721653e+00,  2.9045898e-01, -1.7112731e+00, -1.0120827e+00,\n",
       "        1.1989537e+00, -9.9787414e-02,  8.4593946e-01,  6.7469082e+00,\n",
       "        5.9217703e-01,  2.2312825e+00,  4.0348172e+00,  2.0909085e+00,\n",
       "       -1.0738090e+00,  1.1811446e+00, -6.1611813e-01, -2.7363998e-01,\n",
       "       -6.3329142e-01, -1.0270088e+00, -2.2638702e-01,  1.5096796e-01,\n",
       "        1.6666461e+00, -1.6182579e+00,  5.3560179e-01, -4.9096098e+00,\n",
       "        2.3494067e+00, -6.2780923e-01, -1.9175055e+00,  1.7578743e-01,\n",
       "        2.4549271e-01,  2.4559669e+00,  4.2887168e+00,  1.9157932e+00,\n",
       "       -1.6679986e+00,  1.3591440e+00, -6.0485286e-01,  1.4566317e+00,\n",
       "        2.9361257e-01,  2.1966298e+00,  2.6562362e+00,  1.3344505e-01,\n",
       "        2.8956079e+00, -9.1250902e-01,  3.6828732e-01,  1.2195140e+00,\n",
       "       -1.7470282e+00, -9.3490112e-01, -3.2191554e-01,  1.3363969e-01,\n",
       "        1.2351202e+00, -1.2779020e+00,  1.4146414e+00,  2.0335710e+00,\n",
       "       -7.5099838e-01, -1.0393665e+00,  1.1084979e+00, -1.7224787e+00,\n",
       "       -4.6012342e-01,  2.2935896e+00,  9.8432249e-01, -9.4125086e-01,\n",
       "        3.1345803e-01, -1.1326816e+00, -9.3782067e-01, -2.3491108e+00,\n",
       "       -6.6858852e-01, -3.5770602e+00,  2.6774416e+00,  9.8826504e-01,\n",
       "        8.3321863e-01, -9.0173453e-01,  1.4223164e-01,  4.7399423e-01,\n",
       "       -8.2659543e-01, -6.9357598e-01,  1.6316620e+00,  7.0106089e-01,\n",
       "       -2.6394675e+00,  6.3359147e-01,  3.2140920e+00, -1.2910240e+00,\n",
       "        1.9360977e+00,  7.1833384e-01,  1.0717089e+00, -9.8693752e-01,\n",
       "       -2.3875525e+00,  2.2349246e+00,  2.4133980e+00, -6.8469304e-01,\n",
       "        2.5952747e-01, -3.1188068e+00,  3.4199406e-03, -8.9683428e-02,\n",
       "        1.4847813e+00, -5.5452198e-01,  1.3821489e-01, -8.0653268e-01,\n",
       "       -2.6087945e+00, -2.4263406e+00, -9.7847396e-01,  5.2392650e-01,\n",
       "       -8.4630495e-01,  1.5562367e+00, -5.4305756e-01,  7.5965208e-01,\n",
       "       -1.8037125e+00,  2.9260924e-01, -1.9290141e+00,  7.6528192e-01,\n",
       "        5.6878811e-01, -3.5099279e-02,  2.9947653e+00,  1.9973460e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vec_dic.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>account_category</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[-0.65027815, 1.7860985, -0.09983572, -1.45235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>1D_NICOLE_</td>\n",
       "      <td>Fearmonger</td>\n",
       "      <td>[-0.24397607, 0.56576157, 0.029521903, 0.28719...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1ERIK_LEE</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[-0.14608952, 0.58423156, 0.1004195, 0.0661785...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1LORENAFAVA1</td>\n",
       "      <td>NonEnglish</td>\n",
       "      <td>[-3.2753332, 2.4318128, 1.6246526, 0.36988232,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>2NDHALFONION</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[-0.003391954, 0.0016707064, -0.00091963925, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930731</th>\n",
       "      <td>PLSCALLMEABBY</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[-0.16937305, 0.91246253, 0.51490873, 0.810634...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931021</th>\n",
       "      <td>PLUSHEVYEA</td>\n",
       "      <td>NonEnglish</td>\n",
       "      <td>[-0.0032627394, -0.0017309841, 7.090016e-05, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931252</th>\n",
       "      <td>POLICESTATEME</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>[-1.5918598, 3.4651847, -1.4340701, 1.9369287,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931482</th>\n",
       "      <td>POLIGRAPHME</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[-2.4041529, 2.741313, -2.8414426, 2.6523516, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931896</th>\n",
       "      <td>POLIPROPILENOVA</td>\n",
       "      <td>NonEnglish</td>\n",
       "      <td>[0.00013081092, 0.0024431895, 0.0012675422, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2161 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author account_category  \\\n",
       "0                 10_GOP       RightTroll   \n",
       "424           1D_NICOLE_       Fearmonger   \n",
       "468            1ERIK_LEE       RightTroll   \n",
       "497         1LORENAFAVA1       NonEnglish   \n",
       "3521        2NDHALFONION       RightTroll   \n",
       "...                  ...              ...   \n",
       "2930731    PLSCALLMEABBY          Unknown   \n",
       "2931021       PLUSHEVYEA       NonEnglish   \n",
       "2931252    POLICESTATEME        LeftTroll   \n",
       "2931482      POLIGRAPHME          Unknown   \n",
       "2931896  POLIPROPILENOVA       NonEnglish   \n",
       "\n",
       "                                                       vec  \n",
       "0        [-0.65027815, 1.7860985, -0.09983572, -1.45235...  \n",
       "424      [-0.24397607, 0.56576157, 0.029521903, 0.28719...  \n",
       "468      [-0.14608952, 0.58423156, 0.1004195, 0.0661785...  \n",
       "497      [-3.2753332, 2.4318128, 1.6246526, 0.36988232,...  \n",
       "3521     [-0.003391954, 0.0016707064, -0.00091963925, 0...  \n",
       "...                                                    ...  \n",
       "2930731  [-0.16937305, 0.91246253, 0.51490873, 0.810634...  \n",
       "2931021  [-0.0032627394, -0.0017309841, 7.090016e-05, -...  \n",
       "2931252  [-1.5918598, 3.4651847, -1.4340701, 1.9369287,...  \n",
       "2931482  [-2.4041529, 2.741313, -2.8414426, 2.6523516, ...  \n",
       "2931896  [0.00013081092, 0.0024431895, 0.0012675422, 0....  \n",
       "\n",
       "[2161 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique = dfs[['author', 'account_category']].drop_duplicates()\n",
    "df_unique['vec'] = list(vec_dic.values())\n",
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_unique.loc[df_unique['account_category'].isin(['RightTroll', 'LeftTroll', 'NewsFeed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>account_category</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[-0.65027815, 1.7860985, -0.09983572, -1.45235...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1ERIK_LEE</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[-0.14608952, 0.58423156, 0.1004195, 0.0661785...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>2NDHALFONION</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[-0.003391954, 0.0016707064, -0.00091963925, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>4EVER_SUSAN</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[-0.055862095, 0.3251371, 0.52855974, 1.076397...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>4MYSQUAD</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>[-4.1108074, 1.354283, 1.74966, 0.9054484, 0.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906225</th>\n",
       "      <td>PHOENIXDAILYNEW</td>\n",
       "      <td>NewsFeed</td>\n",
       "      <td>[-0.72074896, 3.8408182, -0.35335833, 2.361104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920984</th>\n",
       "      <td>PIGEONTODAY</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[-0.0922146, 3.55546, 3.3696609, 0.0013206998,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930145</th>\n",
       "      <td>PIPERTRTR</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[0.0033570186, -0.0021629774, -0.0023819532, 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930714</th>\n",
       "      <td>PITTSESTE</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>[0.11633294, 0.355998, 0.13684948, -0.14894602...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931252</th>\n",
       "      <td>POLICESTATEME</td>\n",
       "      <td>LeftTroll</td>\n",
       "      <td>[-1.5918598, 3.4651847, -1.4340701, 1.9369287,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>917 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author account_category  \\\n",
       "0                 10_GOP       RightTroll   \n",
       "468            1ERIK_LEE       RightTroll   \n",
       "3521        2NDHALFONION       RightTroll   \n",
       "5118         4EVER_SUSAN       RightTroll   \n",
       "5181            4MYSQUAD        LeftTroll   \n",
       "...                  ...              ...   \n",
       "2906225  PHOENIXDAILYNEW         NewsFeed   \n",
       "2920984      PIGEONTODAY       RightTroll   \n",
       "2930145        PIPERTRTR       RightTroll   \n",
       "2930714        PITTSESTE       RightTroll   \n",
       "2931252    POLICESTATEME        LeftTroll   \n",
       "\n",
       "                                                       vec  \n",
       "0        [-0.65027815, 1.7860985, -0.09983572, -1.45235...  \n",
       "468      [-0.14608952, 0.58423156, 0.1004195, 0.0661785...  \n",
       "3521     [-0.003391954, 0.0016707064, -0.00091963925, 0...  \n",
       "5118     [-0.055862095, 0.3251371, 0.52855974, 1.076397...  \n",
       "5181     [-4.1108074, 1.354283, 1.74966, 0.9054484, 0.9...  \n",
       "...                                                    ...  \n",
       "2906225  [-0.72074896, 3.8408182, -0.35335833, 2.361104...  \n",
       "2920984  [-0.0922146, 3.55546, 3.3696609, 0.0013206998,...  \n",
       "2930145  [0.0033570186, -0.0021629774, -0.0023819532, 8...  \n",
       "2930714  [0.11633294, 0.355998, 0.13684948, -0.14894602...  \n",
       "2931252  [-1.5918598, 3.4651847, -1.4340701, 1.9369287,...  \n",
       "\n",
       "[917 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df_unique['vec'])\n",
    "y = df_unique['account_category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(penalty='l2')\n",
    "lg.fit(X_train,y_train)\n",
    "y_pred = lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LeftTroll       0.78      0.86      0.82        21\n",
      "    NewsFeed       0.86      0.86      0.86         7\n",
      "  RightTroll       0.97      0.94      0.95        64\n",
      "\n",
      "    accuracy                           0.91        92\n",
      "   macro avg       0.87      0.88      0.88        92\n",
      "weighted avg       0.92      0.91      0.91        92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U2M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Creating node2vec instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing transition probabilities:   0%|                                                   | 0/568789 [00:00<?, ?it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                         | 8/568789 [00:00<1:58:57, 79.69it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 33/568789 [00:00<1:41:32, 93.35it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 49/568789 [00:00<1:29:45, 105.61it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 79/568789 [00:00<1:16:42, 123.56it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 92/568789 [00:00<1:16:09, 124.45it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 125/568789 [00:00<1:01:58, 152.93it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 144/568789 [00:00<1:24:13, 112.53it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 190/568789 [00:01<1:06:48, 141.83it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 212/568789 [00:01<1:06:13, 143.09it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 232/568789 [00:01<1:11:01, 133.41it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 265/568789 [00:01<59:40, 158.77it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 286/568789 [00:01<1:21:35, 116.12it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 303/568789 [00:02<1:28:38, 106.89it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 318/568789 [00:02<1:48:22, 87.43it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 338/568789 [00:02<1:41:34, 93.27it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 374/568789 [00:02<1:19:04, 119.81it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 393/568789 [00:02<1:10:31, 134.33it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 412/568789 [00:02<1:06:50, 141.72it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 459/568789 [00:02<52:58, 178.79it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 502/568789 [00:03<44:46, 211.55it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 532/568789 [00:03<51:07, 185.25it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 565/568789 [00:03<47:42, 198.53it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 590/568789 [00:03<48:34, 194.96it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 613/568789 [00:03<54:30, 173.73it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 665/568789 [00:03<43:39, 216.90it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 695/568789 [00:03<53:42, 176.29it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 720/568789 [00:04<1:09:18, 136.62it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 749/568789 [00:04<1:04:21, 147.11it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 769/568789 [00:04<1:03:50, 148.30it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 787/568789 [00:04<1:05:31, 144.47it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                        | 837/568789 [00:04<51:42, 183.03it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 864/568789 [00:05<1:49:05, 86.76it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 884/568789 [00:05<1:42:32, 92.30it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 901/568789 [00:06<2:05:55, 75.17it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 929/568789 [00:06<1:41:22, 93.36it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 945/568789 [00:06<1:51:23, 84.97it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 959/568789 [00:06<1:46:24, 88.94it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 972/568789 [00:06<1:40:02, 94.60it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 984/568789 [00:06<1:44:44, 90.35it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1012/568789 [00:06<1:25:28, 110.71it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1027/568789 [00:07<1:27:55, 107.63it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1041/568789 [00:07<2:35:14, 60.95it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1057/568789 [00:07<2:07:48, 74.04it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1084/568789 [00:07<1:48:09, 87.48it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1114/568789 [00:07<1:28:46, 106.58it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1130/568789 [00:08<1:22:45, 114.32it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1145/568789 [00:08<1:25:55, 110.11it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1159/568789 [00:08<1:37:06, 97.42it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1180/568789 [00:08<1:24:07, 112.46it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1201/568789 [00:08<1:12:38, 130.22it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1217/568789 [00:08<1:11:15, 132.74it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1233/568789 [00:08<1:13:35, 128.55it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1250/568789 [00:08<1:10:55, 133.36it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1265/568789 [00:13<15:20:15, 10.28it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1276/568789 [00:14<16:21:28,  9.64it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1293/568789 [00:16<14:43:46, 10.70it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1305/568789 [00:16<11:01:18, 14.30it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1354/568789 [00:16<7:49:56, 20.12it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1371/568789 [00:16<5:50:55, 26.95it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1387/568789 [00:16<4:29:30, 35.09it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1402/568789 [00:16<3:33:49, 44.23it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1416/568789 [00:16<3:09:24, 49.92it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1428/568789 [00:17<3:03:34, 51.51it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1438/568789 [00:17<2:47:32, 56.44it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1447/568789 [00:17<3:23:54, 46.37it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1474/568789 [00:17<2:35:57, 60.63it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1486/568789 [00:17<2:13:12, 70.98it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1498/568789 [00:18<2:26:53, 64.37it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1508/568789 [00:18<2:26:27, 64.55it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1517/568789 [00:18<3:14:25, 48.63it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1525/568789 [00:18<4:02:23, 39.00it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1532/568789 [00:18<3:40:53, 42.80it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1538/568789 [00:19<3:38:00, 43.37it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1559/568789 [00:19<2:48:43, 56.03it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1569/568789 [00:19<2:30:42, 62.73it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1584/568789 [00:19<2:07:14, 74.30it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1611/568789 [00:19<1:43:09, 91.64it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1625/568789 [00:19<1:45:55, 89.23it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                      | 1637/568789 [00:19<1:46:55, 88.40it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1654/568789 [00:19<1:33:08, 101.48it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1704/568789 [00:20<1:11:16, 132.60it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1727/568789 [00:20<1:05:52, 143.46it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                     | 1749/568789 [00:20<1:02:26, 151.37it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 1769/568789 [00:20<58:07, 162.59it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|                                       | 1800/568789 [00:20<51:26, 183.68it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                      | 1834/568789 [00:20<44:41, 211.43it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                      | 1865/568789 [00:20<43:38, 216.47it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                      | 1906/568789 [00:20<42:53, 220.32it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                      | 1930/568789 [00:21<58:50, 160.57it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                      | 1950/568789 [00:21<57:32, 164.17it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                     | 1970/568789 [00:21<1:35:05, 99.34it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2011/568789 [00:21<1:14:46, 126.34it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2048/568789 [00:21<1:00:25, 156.33it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                      | 2074/568789 [00:22<54:54, 172.00it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                      | 2099/568789 [00:22<55:24, 170.46it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                     | 2121/568789 [00:22<1:38:10, 96.21it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2141/568789 [00:22<1:30:41, 104.14it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2162/568789 [00:22<1:17:21, 122.08it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2180/568789 [00:29<18:17:03,  8.61it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2193/568789 [00:35<34:07:05,  4.61it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2202/568789 [00:39<45:42:09,  3.44it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2209/568789 [00:42<53:16:08,  2.95it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2214/568789 [00:45<61:26:16,  2.56it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2218/568789 [00:47<65:04:27,  2.42it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2221/568789 [00:48<60:31:39,  2.60it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2223/568789 [00:49<65:10:18,  2.41it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2225/568789 [00:50<74:56:22,  2.10it/s]\u001b[A\n",
      "Computing transition probabilities:   0%|â–                                    | 2226/568789 [00:51<98:17:02,  1.60it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-6d670c085992>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Generate walks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mU2M_node2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNode2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mU2M\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_folder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'temp_folder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\node2vec\\node2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, dimensions, walk_length, num_walks, p, q, weight_key, workers, sampling_strategy, quiet, temp_folder)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sharedmem\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_precompute_probabilities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_walks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\node2vec\\node2vec.py\u001b[0m in \u001b[0;36m_precompute_probabilities\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    102\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mdestination\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Backwards probability\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                         \u001b[0mss_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_node\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[1;32melif\u001b[0m \u001b[0mdestination\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# If the neighbor is connected to the source\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                         \u001b[0mss_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcurrent_node\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_key\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\_collections_abc.py\u001b[0m in \u001b[0;36m__contains__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\classes\\coreviews.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_atlas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate walks\n",
    "U2M_node2vec = Node2Vec(U2M, dimensions=128, walk_length=10, num_walks=10, temp_folder='temp_folder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn embeddings \n",
    "U2M_model = U2M_node2vec.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Inspect the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node, _ in U2M_model.most_similar('@TeamYouTube'):\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"U2H_model.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Uncomment to download \"stopwords\"\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_preprocessing(s):\n",
    "    \"\"\"\n",
    "    - Lowercase the sentence\n",
    "    - Change \"'t\" to \"not\"\n",
    "    - Remove \"@name\"\n",
    "    - Isolate and remove punctuations except \"?\"\n",
    "    - Remove other special characters\n",
    "    - Remove stop words except \"not\" and \"can\"\n",
    "    - Remove trailing whitespace\n",
    "    \"\"\"\n",
    "    s = s.lower()\n",
    "    # Change 't to 'not'\n",
    "    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "    # Remove @name\n",
    "    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "    # Isolate and remove punctuations except '?'\n",
    "    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "    # Remove some special characters\n",
    "    s = re.sub(r'([\\;\\:\\|â€¢Â«\\n])', ' ', s)\n",
    "    # Remove stopwords except 'not' and 'can'\n",
    "#     s = \" \".join([word for word in s.split()\n",
    "#                   if word not in stopwords.words('english')\n",
    "#                   or word in ['not', 'can']])\n",
    "    # Remove trailing whitespace\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>account_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10_GOP</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>RightTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931477</th>\n",
       "      <td>POLICESTATEME</td>\n",
       "      <td>DUDE DROPS COP  #bloodmoon #supermoon #depress...</td>\n",
       "      <td>LeftTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931478</th>\n",
       "      <td>POLICESTATEME</td>\n",
       "      <td>The Police against the crowd  #raw #love #LOL ...</td>\n",
       "      <td>LeftTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931479</th>\n",
       "      <td>POLICESTATEME</td>\n",
       "      <td>The choice is yours  #got7 #FifthHarmony #self...</td>\n",
       "      <td>LeftTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931480</th>\n",
       "      <td>POLICESTATEME</td>\n",
       "      <td>F*ck the police!  #edit #relatable #anime #edi...</td>\n",
       "      <td>LeftTroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931481</th>\n",
       "      <td>POLICESTATEME</td>\n",
       "      <td>Cop Shoots Man Reaching For Drivers License!!!...</td>\n",
       "      <td>LeftTroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1723687 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author                                            content  \\\n",
       "0               10_GOP  \"We have a sitting Democrat US Senator on tria...   \n",
       "1               10_GOP  Marshawn Lynch arrives to game in anti-Trump s...   \n",
       "2               10_GOP  Daughter of fallen Navy Sailor delivers powerf...   \n",
       "3               10_GOP  JUST IN: President Trump dedicates Presidents ...   \n",
       "4               10_GOP  19,000 RESPECTING our National Anthem! #StandF...   \n",
       "...                ...                                                ...   \n",
       "2931477  POLICESTATEME  DUDE DROPS COP  #bloodmoon #supermoon #depress...   \n",
       "2931478  POLICESTATEME  The Police against the crowd  #raw #love #LOL ...   \n",
       "2931479  POLICESTATEME  The choice is yours  #got7 #FifthHarmony #self...   \n",
       "2931480  POLICESTATEME  F*ck the police!  #edit #relatable #anime #edi...   \n",
       "2931481  POLICESTATEME  Cop Shoots Man Reaching For Drivers License!!!...   \n",
       "\n",
       "        account_category  \n",
       "0             RightTroll  \n",
       "1             RightTroll  \n",
       "2             RightTroll  \n",
       "3             RightTroll  \n",
       "4             RightTroll  \n",
       "...                  ...  \n",
       "2931477        LeftTroll  \n",
       "2931478        LeftTroll  \n",
       "2931479        LeftTroll  \n",
       "2931480        LeftTroll  \n",
       "2931481        LeftTroll  \n",
       "\n",
       "[1723687 rows x 3 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df = dfs[['author', 'content', 'account_category']].loc[dfs['account_category'].isin(['RightTroll', 'LeftTroll', 'NewsFeed'])]\n",
    "bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          we have a sitting democrat us senator on trial...\n",
       "1          marshawn lynch arrives to game in anti trump s...\n",
       "2          daughter of fallen navy sailor delivers powerf...\n",
       "3          just in president trump dedicates presidents c...\n",
       "4          19 000 respecting our national anthem standfor...\n",
       "                                 ...                        \n",
       "2931477    dude drops cop bloodmoon supermoon depression ...\n",
       "2931478    the police against the crowd raw love lol funn...\n",
       "2931479    the choice is yours got7 fifthharmony selfiefo...\n",
       "2931480    f ck the police edit relatable anime edit lol ...\n",
       "2931481    cop shoots man reaching for drivers license co...\n",
       "Name: content, Length: 1723687, dtype: object"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df['content'].apply(text_preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in tqdm(data, position=0):\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,      # Return attention mask\n",
    "            truncation=True\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bert_df['content']\n",
    "y = df_unique['account_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the model we want to use\n",
    "MODEL_NAME = \"bert-large-uncased\"\n",
    "\n",
    "# We need to create the model and tokenizer\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1723482/1723687 [10:52<00:00, 3214.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1723687/1723687 [10:52<00:00, 2640.27it/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in tqdm(bert_df['content'], position=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  347\n"
     ]
    }
   ],
   "source": [
    "print('Max length: ', max([len(sent) for sent in encoded_tweets]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 91.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  \"We have a sitting Democrat US Senator on trial for corruption and you've barely heard a peep from the mainstream media.\" ~ @nedryun https://t.co/gh6g0D1oiC\n",
      "Token IDs:  [101, 2057, 2031, 1037, 3564, 7672, 2149, 5205, 2006, 3979, 2005, 7897, 1998, 2017, 2310, 4510, 2657, 1037, 21392, 2361, 2013, 1996, 7731, 2865, 16770, 1056, 2522, 1043, 2232, 2575, 2290, 2692, 2094, 2487, 19419, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 165/1723687 [00:00<17:32, 1638.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1723687/1723687 [14:04<00:00, 2041.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "X_inputs, X_masks = preprocessing_for_bert(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1723687/1723687 [00:21<00:00, 81454.06it/s]\n"
     ]
    }
   ],
   "source": [
    "bert_dic = dict()\n",
    "for i in tqdm(range(bert_df.shape[0]), position=0):\n",
    "    bert_dic.setdefault(bert_df.iloc[i, 0], []).append(X_inputs[i].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaging the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in bert_dic:\n",
    "    bert_dic[k] = np.mean(bert_dic[k], keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatting the vector list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (k, v) in enumerate(bert_dic.items()):\n",
    "    bert_dic[k] = [item for sublist in list(bert_dic.values())[i] for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(bert_dic.values())\n",
    "y = df_unique['account_category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(penalty='l2')\n",
    "lg.fit(X_train,y_train)\n",
    "y_pred = lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   LeftTroll       0.29      0.15      0.20        67\n",
      "    NewsFeed       0.56      0.28      0.37        18\n",
      "  RightTroll       0.71      0.86      0.78       191\n",
      "\n",
      "    accuracy                           0.65       276\n",
      "   macro avg       0.52      0.43      0.45       276\n",
      "weighted avg       0.60      0.65      0.61       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
